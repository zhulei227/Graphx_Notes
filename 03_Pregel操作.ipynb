{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上一节提出的三个问题，可以通过仅调用一次Pregel算子就可以解决，下面先利用Pregel计算一次上节的最远距离，然后再说明各参数的含义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sparkConf = org.apache.spark.SparkConf@78edcfc0\n",
       "sc = org.apache.spark.SparkContext@7e3c0318\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.SparkContext@7e3c0318"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.{SparkConf, SparkContext}\n",
    "import org.apache.spark.graphx._\n",
    "val sparkConf: SparkConf = new SparkConf().setMaster(\"local[2]\")\n",
    "val sc: SparkContext = new SparkContext(sparkConf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "verticesRDD = ParallelCollectionRDD[0] at parallelize at <console>:32\n",
       "edgesRDD = ParallelCollectionRDD[1] at parallelize at <console>:33\n",
       "graph = org.apache.spark.graphx.impl.GraphImpl@785c63fa\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.graphx.impl.GraphImpl@785c63fa"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//为了方便，这里各节点属性就直接设置为0了\n",
    "val verticesRDD=sc.parallelize(Array((0L,0),(1L,0),(2L,0),(3L,0),(4L,0),(5L,0)))\n",
    "val edgesRDD=sc.parallelize(Array(Edge(0L,1L,\"r1\"),Edge(0L,2L,\"r1\"),Edge(1L,3L,\"r2\"),Edge(2L,3L,\"r3\"),Edge(2L,4L,\"r2\"),Edge(5L,4L,\"r3\")))\n",
    "val graph=Graph(verticesRDD,edgesRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "finalGraph = org.apache.spark.graphx.impl.GraphImpl@58b17522\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.graphx.impl.GraphImpl@58b17522"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//调用pregel\n",
    "val finalGraph=Pregel(graph=graph,initialMsg=0,maxIterations=10,activeDirection=EdgeDirection.Out)(\n",
    "    vprog=(_,vd,aggMsg)=>math.max(vd,aggMsg),\n",
    "    sendMsg=(edge)=>if(edge.srcAttr+1>edge.dstAttr) Iterator((edge.dstId,edge.srcAttr+1)) else Iterator.empty,\n",
    "    mergeMsg=(a,b)=>math.max(a,b)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array((4,2), (0,0), (2,1), (1,1), (3,2), (5,0))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finalGraph.vertices.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "Compile Error",
     "evalue": "<console>:1: error: illegal character '\\uff0c'\n可以发现比其上一节的计算要方便很多，说明一下参数含义：\n                 ^\n<console>:1: error: illegal character '\\uff1a'\n可以发现比其上一节的计算要方便很多，说明一下参数含义：\n                          ^\n<console>:3: error: illegal character '\\uff0c'\ninitialMsg:初始是发送给各节点的消息，它在第一轮调用是对应了vprog函数中的aggMsg;\n                       ^\n<console>:5: error: illegal character '\\uff0c'\nactiveDirection:消息传输方向为出度方向，此外还有In,Either,Both这些在后续的算法调用中再介绍;\n                           ^\n<console>:7: error: illegal character '\\uff0c'\nsendMsg:与aggerateMessages中类似，不过需要在外面套一层Iterator，并指定发送到dstId;\n                            ^\n<console>:7: error: illegal character '\\uff0c'\nsendMsg:与aggerateMessages中类似，不过需要在外面套一层Iterator，并指定发送到dstId;\n                                               ^\n<console>:10: error: illegal character '\\uff0c'\n这里，迭代收敛条件似乎是由maxIterations控制的，其实不然，迭代收敛条件还受到sendMsg函数影响，如果某次调用所有边的消息都为Iterator.empty,那么迭代也会终止\n  ^\n<console>:10: error: illegal character '\\uff0c'\n这里，迭代收敛条件似乎是由maxIterations控制的，其实不然，迭代收敛条件还受到sendMsg函数影响，如果某次调用所有边的消息都为Iterator.empty,那么迭代也会终止\n                             ^\n<console>:10: error: illegal character '\\uff0c'\n这里，迭代收敛条件似乎是由maxIterations控制的，其实不然，迭代收敛条件还受到sendMsg函数影响，如果某次调用所有边的消息都为Iterator.empty,那么迭代也会终止\n                                  ^\n<console>:10: error: illegal character '\\uff0c'\n这里，迭代收敛条件似乎是由maxIterations控制的，其实不然，迭代收敛条件还受到sendMsg函数影响，如果某次调用所有边的消息都为Iterator.empty,那么迭代也会终止\n                                                       ^\n",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "可以发现比其上一节的计算要方便很多，说明一下参数含义：   \n",
    "\n",
    "initialMsg:初始是发送给各节点的消息，它在第一轮调用是对应了vprog函数中的aggMsg;  \n",
    "maxIterations:最大迭代次数;  \n",
    "activeDirection:消息传输方向为出度方向，此外还有In,Either,Both这些在后续的算法调用中再介绍;  \n",
    "vprog:即处理聚合后的消息与原节点属性的函数;  \n",
    "sendMsg:与aggerateMessages中类似，不过需要在外面套一层Iterator，并指定发送到dstId;  \n",
    "mergeMsg:与aggrateMessages中一样   \n",
    "\n",
    "注意，迭代收敛条件似乎是由maxIterations控制的，其实不止，迭代收敛条件还受到sendMsg函数影响，如果某次调用所有边的消息都为Iterator.empty,那么迭代也会终止"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark2.4.8 - Scala",
   "language": "scala",
   "name": "spark2.4.8_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
