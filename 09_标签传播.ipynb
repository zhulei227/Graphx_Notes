{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spark实现标签传播算法的流程如下：   \n",
    "\n",
    "0.将每个顶点ID设置为其初始标签；   \n",
    "1.每个顶点将其标签发送给所有邻居；   \n",
    "2.将所有邻居的标签进行统计，将出现次数最多的标签更新为当前顶点的新标签；  \n",
    "\n",
    "重复1～2步N次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sparkConf = org.apache.spark.SparkConf@7b107f2a\n",
       "sc = org.apache.spark.SparkContext@4d82496\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.SparkContext@4d82496"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.{SparkConf, SparkContext}\n",
    "import org.apache.spark.graphx._\n",
    "val sparkConf: SparkConf = new SparkConf().setMaster(\"local[2]\")\n",
    "val sc: SparkContext = new SparkContext(sparkConf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "initGraph = org.apache.spark.graphx.impl.GraphImpl@ef1b04e\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.graphx.impl.GraphImpl@ef1b04e"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val initGraph=util.GraphGenerators.rmatGraph(sc,32,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lpaGraph = org.apache.spark.graphx.impl.GraphImpl@604c94f0\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.graphx.impl.GraphImpl@604c94f0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//将顶点ID设置为标签\n",
    "val lpaGraph = initGraph.mapVertices{case (vid, _) => vid }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sendMessage: (e: org.apache.spark.graphx.EdgeTriplet[org.apache.spark.graphx.VertexId,Int])Iterator[(org.apache.spark.graphx.VertexId, Map[org.apache.spark.graphx.VertexId,Long])]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "//向邻居发送自己的标签号\n",
    "def sendMessage(e: EdgeTriplet[VertexId, Int]): Iterator[(VertexId, Map[VertexId, Long])] = {\n",
    "      Iterator((e.srcId, Map(e.dstAttr -> 1L)), (e.dstId, Map(e.srcAttr -> 1L)))\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mergeMessage: (count1: scala.collection.Map[org.apache.spark.graphx.VertexId,Long], count2: scala.collection.Map[org.apache.spark.graphx.VertexId,Long])scala.collection.Map[org.apache.spark.graphx.VertexId,Long]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "//统计标签数\n",
    "import scala.collection.{mutable, Map}\n",
    "def mergeMessage(count1: Map[VertexId, Long], count2: Map[VertexId, Long])\n",
    "      : Map[VertexId, Long] = {\n",
    "      // Mimics the optimization of breakOut, not present in Scala 2.13, while working in 2.12\n",
    "      val map = mutable.Map[VertexId, Long]()\n",
    "      (count1.keySet ++ count2.keySet).foreach { i =>\n",
    "        val count1Val = count1.getOrElse(i, 0L)\n",
    "        val count2Val = count2.getOrElse(i, 0L)\n",
    "        map.put(i, count1Val + count2Val)\n",
    "      }\n",
    "      map\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vertexProgram: (vid: org.apache.spark.graphx.VertexId, attr: Long, message: scala.collection.Map[org.apache.spark.graphx.VertexId,Long])org.apache.spark.graphx.VertexId\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "//将邻居中标签占比最高的标签更新为当前节点的标签\n",
    "def vertexProgram(vid: VertexId, attr: Long, message: Map[VertexId, Long]): VertexId = {\n",
    "      if (message.isEmpty) attr else message.maxBy(_._2)._1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "initialMessage = Map()\n",
       "maxSteps = 10\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val initialMessage = Map[VertexId, Long]()\n",
    "val maxSteps=10\n",
    "// Pregel(lpaGraph, initialMessage, maxIterations = maxSteps)(\n",
    "//       vprog = vertexProgram,\n",
    "//       sendMsg = sendMessage,\n",
    "//       mergeMsg = mergeMessage)\n",
    "//这里如果是单机运行会有序列化问题，可以这样解决：1）集群环境不会有问题  2）将上面的仨函数直接隐式的写到pregel里面"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lpa缺点很多，主要俩点：   \n",
    "\n",
    "1.需要自己设置最大迭代次数（上面的maxSteps），如果不设置，最后可能所有的顶点都是同一个标签；   \n",
    "2.由于初始轮，所有邻居标签都只出现过一次，其实等价于从邻居标签中随机选择一个，这样会造成最终结果很不稳定，重复运行代码，每次的结果差异都会很大;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark2.4.8 - Scala",
   "language": "scala",
   "name": "spark2.4.8_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
